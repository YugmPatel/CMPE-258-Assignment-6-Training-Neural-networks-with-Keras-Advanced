# -*- coding: utf-8 -*-
"""1_Keras_Data_Augmentation_Yugm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Tj9zA0sCe2VmsXNZQ5uLa1hEVO874mg

## ðŸ§ª Task (a): L1 and L2 Regularization

In this task, we analyze and compare the impact of **L1** and **L2** regularization on model performance and weight behavior using synthetic regression data.

### ðŸ” Key Observations:

- **L1 Regularization**  
  Encourages **sparsity** by driving some weights to exactly zero.  
  This often acts as a form of **automatic feature selection**.

- **L2 Regularization**  
  Penalizes large weights but keeps most of them small and non-zero.  
  Promotes **smoothness and stability** in weight distribution.

### ðŸ§ª A/B Test Setup:
We compare three models:
1. **Baseline (No Regularization)**
2. **L1 Regularized Model**
3. **L2 Regularized Model**

ðŸ“Š We use synthetic regression data for clearer interpretation of weight behaviors and regularization effects.
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
X = np.linspace(-3, 3, 100)
y = 0.5 * X + np.sin(X) + np.random.normal(scale=0.3, size=X.shape)

X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

# Build and train models
def build_model(reg_type=None):
    if reg_type == 'l1':
        reg = tf.keras.regularizers.l1(0.01)
    elif reg_type == 'l2':
        reg = tf.keras.regularizers.l2(0.01)
    else:
        reg = None
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=reg, input_shape=(1,)),
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=reg),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

histories = {}
for name in ['none', 'l1', 'l2']:
    model = build_model(None if name == 'none' else name)
    history = model.fit(X, y, epochs=100, verbose=0)
    histories[name] = (model, history)

# Visualization
plt.figure(figsize=(12, 6))
for name, (model, history) in histories.items():
    plt.plot(history.history['loss'], label=f'{name.upper()} Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss Comparison (No Reg vs L1 vs L2)")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Model with and without dropout
def build_dropout_model(use_dropout=False):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)))
    if use_dropout:
        model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    if use_dropout:
        model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

model_dropout = build_dropout_model(True)
model_no_dropout = build_dropout_model(False)

hist_drop = model_dropout.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)
hist_no_drop = model_no_dropout.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)

# Plotting validation loss
plt.figure(figsize=(12, 6))
plt.plot(hist_no_drop.history['val_loss'], label='No Dropout')
plt.plot(hist_drop.history['val_loss'], label='With Dropout')
plt.xlabel("Epochs")
plt.ylabel("Validation Loss")
plt.title("Dropout Regularization: Validation Loss Comparison")
plt.legend()
plt.grid(True)
plt.show()

# Build a simple model
def build_early_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# Without EarlyStopping
model_no_es = build_early_model()
history_no_es = model_no_es.fit(X_train, y_train, validation_data=(X_test, y_test),
                                 epochs=100, verbose=0)

# With EarlyStopping
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_es = build_early_model()
history_es = model_es.fit(X_train, y_train, validation_data=(X_test, y_test),
                          epochs=100, callbacks=[early_stop], verbose=0)

# Visualization
plt.figure(figsize=(12, 6))
plt.plot(history_no_es.history['val_loss'], label='No EarlyStopping')
plt.plot(history_es.history['val_loss'], label='With EarlyStopping')
plt.xlabel("Epochs")
plt.ylabel("Validation Loss")
plt.title("Effect of EarlyStopping on Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

class MCDropoutModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.d1 = tf.keras.layers.Dense(64, activation='relu')
        self.dropout1 = tf.keras.layers.Dropout(0.3)
        self.d2 = tf.keras.layers.Dense(64, activation='relu')
        self.dropout2 = tf.keras.layers.Dropout(0.3)
        self.out = tf.keras.layers.Dense(1)

    def call(self, x, training=False):
        x = self.d1(x)
        x = self.dropout1(x, training=training)
        x = self.d2(x)
        x = self.dropout2(x, training=training)
        return self.out(x)

# Train the model
mc_model = MCDropoutModel()
mc_model.compile(optimizer='adam', loss='mse')
mc_model.fit(X_train, y_train, epochs=100, verbose=0)

# Predict with dropout (MC sampling)
X_sample = np.linspace(-3, 3, 100).reshape(-1, 1)
predictions = [mc_model(X_sample, training=True).numpy().flatten() for _ in range(100)]

# Visualization: Mean + uncertainty bands
mean_preds = np.mean(predictions, axis=0)
std_preds = np.std(predictions, axis=0)

plt.figure(figsize=(12, 6))
plt.plot(X_sample, mean_preds, label='Mean Prediction', color='green')
plt.fill_between(X_sample.flatten(), mean_preds - std_preds, mean_preds + std_preds, alpha=0.3, label='Â±1 std')
plt.scatter(X_train, y_train, s=10, label='Train Data', alpha=0.6)
plt.title("Monte Carlo Dropout Predictions with Uncertainty")
plt.xlabel("Input")
plt.ylabel("Prediction")
plt.legend()
plt.grid(True)
plt.show()

# Build models with different initializations
def build_init_model(initializer):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer, input_shape=(1,)),
        tf.keras.layers.Dense(64, activation='relu', kernel_initializer=initializer),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

inits = {
    "RandomNormal": tf.keras.initializers.RandomNormal(),
    "HeNormal": tf.keras.initializers.HeNormal(),
    "GlorotUniform": tf.keras.initializers.GlorotUniform()
}

histories_init = {}
for name, init in inits.items():
    model = build_init_model(init)
    hist = model.fit(X_train, y_train, epochs=100, verbose=0)
    histories_init[name] = hist

# Visualization
plt.figure(figsize=(12, 6))
for name, hist in histories_init.items():
    plt.plot(hist.history['loss'], label=f'{name} Init')
plt.title("Effect of Different Initializations on Training Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Models with and without batch normalization
def build_bn_model(use_bn=False):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(64, input_shape=(1,)))
    if use_bn:
        model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation('relu'))

    model.add(tf.keras.layers.Dense(64))
    if use_bn:
        model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Activation('relu'))

    model.add(tf.keras.layers.Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

model_with_bn = build_bn_model(True)
model_without_bn = build_bn_model(False)

hist_bn = model_with_bn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)
hist_no_bn = model_without_bn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)

# Visualization
plt.figure(figsize=(12, 6))
plt.plot(hist_bn.history['val_loss'], label='With BatchNorm')
plt.plot(hist_no_bn.history['val_loss'], label='Without BatchNorm')
plt.title("Effect of Batch Normalization on Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

# Custom Dropout Layer
class MyDropout(tf.keras.layers.Layer):
    def __init__(self, rate):
        super().__init__()
        self.rate = rate

    def call(self, inputs, training=False):
        if training:
            mask = tf.random.uniform(tf.shape(inputs)) >= self.rate
            return tf.where(mask, inputs, tf.zeros_like(inputs))
        return inputs

# Custom Regularizer (L1 + log penalty)
class CustomReg(tf.keras.regularizers.Regularizer):
    def __init__(self, l1=0.01):
        self.l1 = l1

    def __call__(self, x):
        return self.l1 * tf.reduce_sum(tf.abs(x)) + 0.01 * tf.reduce_sum(tf.math.log(1 + tf.square(x)))

# Model using custom layers
def build_custom_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=CustomReg(), input_shape=(1,)),
        MyDropout(0.3),
        tf.keras.layers.Dense(64, activation='relu'),
        MyDropout(0.3),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

custom_model = build_custom_model()
history_custom = custom_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)

# Visualization
plt.figure(figsize=(12, 6))
plt.plot(history_custom.history['val_loss'], label='Custom Dropout & Reg')
plt.title("Custom Regularization and Dropout: Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.grid(True)
plt.legend()
plt.show()

import os
import datetime

# Define log directory
log_dir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

# TensorBoard + Callbacks
tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, verbose=1)
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("best_model.h5", save_best_only=True)

# Model for training
model_cb = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])
model_cb.compile(optimizer='adam', loss='mse')

# Train with callbacks
model_cb.fit(X_train, y_train,
             validation_data=(X_test, y_test),
             epochs=100,
             callbacks=[tensorboard_cb, reduce_lr_cb, checkpoint_cb],
             verbose=0)

# Commented out IPython magic to ensure Python compatibility.
# View TensorBoard (run in separate cell)
# %load_ext tensorboard
# %tensorboard --logdir logs

!pip install -q -U keras-tuner

import keras_tuner as kt

# Define a HyperModel class
def build_model(hp):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(
        units=hp.Int('units', min_value=32, max_value=128, step=32),
        activation='relu',
        input_shape=(1,)
    ))
    model.add(tf.keras.layers.Dense(1))
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Float('lr', 1e-4, 1e-2, sampling='log')),
        loss='mse'
    )
    return model

# Instantiate tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=5,
    executions_per_trial=2,
    directory='my_tuner_dir',
    project_name='regression_tuning'
)

# Search best hyperparameters
tuner.search(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)

# Visualize and summarize
best_model = tuner.get_best_models(1)[0]
tuner.results_summary()

# Plot predictions
y_pred = best_model.predict(X_test)
plt.figure(figsize=(10, 5))
plt.scatter(X_test, y_test, label='True', alpha=0.6)
plt.scatter(X_test, y_pred, label='Predicted', alpha=0.6)
plt.title("Best Model Prediction (Tuned by Keras Tuner)")
plt.legend()
plt.grid(True)
plt.show()

!pip install -q -U keras-cv
import keras_cv
import tensorflow_datasets as tfds

# Load small dataset (e.g., cats_vs_dogs)
(ds_train, ds_val), ds_info = tfds.load('cats_vs_dogs',
                                        split=['train[:80%]', 'train[80%:]'],
                                        as_supervised=True,
                                        with_info=True)

# Preprocess + resize
def format_image(image, label):
    image = tf.image.resize(image, (224, 224)) / 255.0
    return image, label

ds_train = ds_train.map(format_image).batch(16).prefetch(tf.data.AUTOTUNE)
ds_val = ds_val.map(format_image).batch(16).prefetch(tf.data.AUTOTUNE)

# Apply KerasCV augmentations
augmenter = tf.keras.Sequential([
    keras_cv.layers.RandomFlip(mode="horizontal"),
    keras_cv.layers.RandomRotation(0.2),
    keras_cv.layers.RandomZoom(height_factor=0.1, width_factor=0.1)
])

# Visualize augmentations
sample_images, _ = next(iter(ds_train))
augmented_images = augmenter(sample_images)

plt.figure(figsize=(12, 6))
for i in range(6):
    plt.subplot(2, 6, i + 1)
    plt.imshow(sample_images[i])
    plt.axis("off")
    plt.title("Original")

    plt.subplot(2, 6, i + 7)
    plt.imshow(augmented_images[i])
    plt.axis("off")
    plt.title("Augmented")
plt.suptitle("KerasCV Image Augmentation")
plt.tight_layout()
plt.show()

!pip install augly augly[av]

!sudo apt-get install python3-magic

import os
import augly.image as imaugs
import augly.utils as utils
from IPython.display import display
from google.colab import files
from PIL import Image
import io

# Prompt user to upload an image
uploaded = files.upload()

# Load the uploaded image
for fn in uploaded.keys():
    input = Image.open(io.BytesIO(uploaded[fn]))
    print(f"Uploaded image: {fn}")
    break  # Process only the first uploaded image

image = imaugs.scale(input, factor=0.1)
display(image)

image = imaugs.blur(input, radius = 4.0)
display(image)

image = imaugs.brightness(input, factor=1.2)
display(image)

image = imaugs.brightness(input, factor=0.3)
display(image)

image = imaugs.contrast(input, factor=2.7)
display(image)

#horizontal flip
image_h = imaugs.hflip(input)
display(image_h)

#vertical flip
image_v= imaugs.vflip(input)
display(image_v)

#Grayscale image
image = imaugs.grayscale(input)
display(image)

# Degrading Image Pixels
image = imaugs.shuffle_pixels(input, factor=0.3)
display(image)

image = imaugs.skew(input)
display(image)

! [ -e /content ] && pip install -Uqq fastbook
import fastbook
fastbook.setup_book()

from fastbook import *
from fastai.vision.all import *
path = untar_data(URLs.IMAGENETTE)

dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),
                   get_items=get_image_files,
                   get_y=parent_label,
                   item_tfms=Resize(460),
                   batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = dblock.dataloaders(path, bs=64)

from fastai.vision.all import *

# Load and resize images (using different categories)
# Replace with other image indices from the respective categories
image1_path = get_image_files_sorted(path/'train'/'n01440764')[30]  # Example: tench, image index 30
image2_path = get_image_files_sorted(path/'train'/'n02102040')[150] # Example: English springer, image index 150

image1 = PILImage.create(image1_path).resize((256, 256))
image2 = PILImage.create(image2_path).resize((256, 256))

# Convert to tensors and normalize
tensor1 = tensor(image1).float() / 255.
tensor2 = tensor(image2).float() / 255.

# Display images
_, axs = plt.subplots(1, 3, figsize=(12, 4))
show_image(tensor1, ax=axs[0]);
show_image(tensor2, ax=axs[1]);
show_image((0.3 * tensor1 + 0.7 * tensor2), ax=axs[2]);